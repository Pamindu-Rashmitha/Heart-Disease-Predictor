# -*- coding: utf-8 -*-
"""Heart-Risk-Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-9XcgrrN8jogaVs0JEsaNi8EqYJ4GvYN

#Upload the CSV
"""

from google.colab import files
uploaded = files.upload()

"""#Load and inspect data"""

import pandas as pd

df = pd.read_csv('heart.csv')
df.head()

df.describe()

df.info()

"""#Preprocessing and EDA

* No null values therefore no need to handle missing values.
* No categorical variables hence no enoding needed

##Age analysis
"""

import seaborn as sns

sns.histplot(df['age'])

sns.boxplot(df['age'])

"""##Sex Analysis"""

print(df['sex'].value_counts())

"""*   0 - female
*   1 - male

##Chest pain type (CP) Analysis
"""

print(df['cp'].value_counts())

df['cp'].value_counts().plot(kind='pie')

"""##Resting blood pressure (trestbps) Analysis"""

sns.histplot(df['trestbps'])

sns.boxplot(df['trestbps'])

"""
*   There are some outliers present"""

Q1 = df['trestbps'].quantile(0.25)
Q3 = df['trestbps'].quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = (df['trestbps'] < lower_bound) | (df['trestbps'] > upper_bound)
outliers.value_counts()

"""* 30 outliers present with 995 no outliers
* we can ignore them since the ratio is too small

##Serum cholesterol (chol) Analysis
"""

sns.histplot(df['chol'])

sns.boxplot(df['chol'])

Q1 = df['chol'].quantile(0.25)
Q3 = df['chol'].quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = (df['chol'] < lower_bound) | (df['chol'] > upper_bound)
outliers.value_counts()

"""* safe to ignore since 16 outliers present with 1009 non outliers

##Fasting blood sugar (fbs) Analysis
"""

print(df['fbs'].value_counts())

df['fbs'].value_counts().plot(kind='pie')

"""##Resting electrocardiographic results (restecg) Analysis"""

print(df['restecg'].value_counts())

df['restecg'].value_counts().plot(kind='bar')

"""##Maximum heart rate achieved (thalach) Analysis

"""

sns.histplot(df['thalach'])

sns.boxplot(df['thalach'])

"""##Exercise induced angina (exang) Analysis"""

print(df['exang'].value_counts())

df['exang'].value_counts().plot(kind='pie')

"""##ST depression induced by exercise relative to rest (oldpeak) Analysis



"""

sns.histplot(df['oldpeak'])

sns.boxplot(df['oldpeak'])

"""##ca Analysis"""

sns.histplot(df['ca'])

sns.boxplot(df['ca'])

Q1 = df['ca'].quantile(0.25)
Q3 = df['ca'].quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = (df['ca'] < lower_bound) | (df['ca'] > upper_bound)
outliers.value_counts()

"""* 87 outliers with 938 non outliers

##thal Analysis
"""

sns.histplot(df['thal'])

sns.boxplot(df['thal'])

"""##Target variable Analysis"""

df['target'].value_counts().plot(kind='bar')

df['target'].value_counts()

"""* The target class is almost perfectly balanced

##Remove Duplicate Rows
"""

print(f"Duplicate rows: {df.duplicated().sum()}")
df = df.drop_duplicates()

print(f"Remaining unique patients: {len(df)}")

"""#Correlation Matrix of features"""

import matplotlib.pyplot as plt

corr_matrix = df.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix,annot=True,cmap='coolwarm',fmt=".2f")
plt.title('Correlation Matrix of Disease Features')
plt.show()

"""* cp(chest pain),thalach(max heart rate) and slope have the strongest positive correlation with the target.

#Feature engineering (Feature selection and dimension reduction)
"""

from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.decomposition import PCA


X = df.drop('target', axis=1)
y = df['target']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Feature Selection (SelectKBest)
# Select the top 5 features using ANOVA F-value
selector = SelectKBest(score_func=f_classif, k=5)
X_selected = selector.fit_transform(X_scaled, y)

# Get the names of the selected columns
selected_features = X.columns[selector.get_support()]
print(f"Top 5 Selected Features: {selected_features.tolist()}")

# Dimensionality Reduction (PCA)
# Reduce 13 features down to 2 principal components
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

print(f"Original Shape: {X_scaled.shape}")
print(f"Reduced Shape:  {X_pca.shape}")

"""##Split the Dataset into Test and Train sets"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""#Scale numerical features"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

# Fit on Training data ONLY (to avoid data leakage)
X_train_scaled = scaler.fit_transform(X_train)

# Transform Test data
X_test_scaled = scaler.transform(X_test)

"""##Visualize Scaled features"""

scaled_cols = ['age','trestbps','chol','thalach','oldpeak']

for cols in scaled_cols:
  sns.histplot(df[cols])
  plt.show()

"""#Model Training"""

import xgboost as xgb
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

models ={
    "Logistic Regression": LogisticRegression(),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "XGBoost": xgb.XGBClassifier(eval_metric='logloss')
}

for name, model in models.items():

    model.fit(X_train_scaled, y_train)

    predictions = model.predict(X_test_scaled)

    accuracy = accuracy_score(y_test, predictions)
    print(f"{name} Accuracy: {accuracy:.4f}")

"""* since the accuracy in Random Forest is high lets hyper parameter tune it to increase performance"""

from sklearn.model_selection import GridSearchCV

# Define the Parameter Grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# 2. Run Grid Search (This finds the best settings automatically)
print("Tuning Random Forest... this may take a moment.")
rf = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train_scaled, y_train)

# 3. Use the Best Model
best_rf = grid_search.best_estimator_
print(f"Best Parameters: {grid_search.best_params_}")

# 4. Predict and Evaluate
predictions = best_rf.predict(X_test_scaled)
acc = accuracy_score(y_test, predictions)
print(f"Tuned Random Forest Accuracy: {acc:.4f}")

"""* There is no difference in the accuracy hence the dataset being too small(Removed many duplicate rows)

##Prediction
"""

import numpy as np

print("\n" + "="*70)
print(" PATIENT RISK REPORT (SAMPLE)")
print("="*70)
print(f"{'Age':<5} {'Sex':<5} {'Chol':<5} {'MaxHR':<5} | {'True Status':<12} {'Prediction':<12} {'Risk Probability'}")
print("-" * 70)

# Pick 5 random patients from the test set
random_indices = np.random.choice(X_test.index, 5, replace=False)

for i in random_indices:
    # Get original data (for display purposes)
    patient_data = X_test.loc[i]
    true_label = "Disease" if y_test.loc[i] == 1 else "Healthy"

    # Get scaled data (for the model to predict)
    # We have to find the location of this patient in the scaled array
    row_index = X_test.index.get_loc(i)
    patient_scaled = X_test_scaled[row_index].reshape(1, -1)

    # 3. Make Prediction
    predicted_class = best_rf.predict(patient_scaled)[0]
    prediction_prob = best_rf.predict_proba(patient_scaled)[0][1]

    pred_label = "Disease" if predicted_class == 1 else "Healthy"

    # 4. Print Row
    print(f"{int(patient_data['age']):<5} {int(patient_data['sex']):<5} {int(patient_data['chol']):<5} {int(patient_data['thalach']):<5} | {true_label:<12} {pred_label:<12} {prediction_prob:.2f}")

print("-" * 70)